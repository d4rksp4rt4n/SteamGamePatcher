name: Update Patches Database
on:
#  schedule:
#   - cron: '0 4 * * *' # Runs daily at 04:00 UTC (1 hour after Script Automation's 03:00)
#   - cron: '0 16 * * *' # Runs daily at 16:00 UTC (1 hour after Script Automation's 15:00)
#   - cron: '0 0 * * 0'  # Weekly: Purge old caches (uncomment if needed)
  workflow_dispatch:
  repository_dispatch:
    types: [patches-updated]  # Triggered from nukige-site changes
  push:
    branches: [ main ]
    paths:
      - '.github/workflows/update-db.yml'
      - 'database/gdrive_db.py'
      - 'database/unify_db.py'
      - 'database/minify_db.py'
jobs:
  build-and-update:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    concurrency:  # Prevent overlaps
      group: update-patches-db-${{ github.ref }}
      cancel-in-progress: true
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Restore cache  # Always restore for incremental start
        id: cache-restore
        uses: actions/cache/restore@v4
        with:
          path: database/data/
          key: drive-state-v6-${{ hashFiles('database/gdrive_db.py', 'database/unify_db.py') }}  # Base key
          restore-keys: |
            drive-state-v6-  # Fallback: Any prior unique save

      - name: Log cache restore
        run: |
          if [ "${{ steps.cache-restore.outputs.cache-hit }}" == 'true' ]; then
            echo "Cache restored (hit: ${{ steps.cache-restore.outputs.cache-primary-key }})"
            ls -la database/data/ || true
          else
            echo "Cache miss (fallback: ${{ steps.cache-restore.outputs.cache-matched-key }})"
          fi
        shell: bash

      - name: Ensure data folder
        run: mkdir -p database/data

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
          
      - name: Install dependencies
        run: |
          pip install google-api-python-client google-auth jq

      - name: Save Service Account JSON
        env:
          SERVICE_ACCOUNT_JSON: ${{ secrets.SERVICE_ACCOUNT_JSON }}
        run: echo "$SERVICE_ACCOUNT_JSON" > service-account.json

      - name: Fetch patches_data.json from PRIVATE nukige-site
        env:
          TOKEN: ${{ secrets.NUKIGE_TOKEN }}
        run: |
          echo "::add-mask::$TOKEN"
          for attempt in {1..3}; do
            API_URL="https://api.github.com/repos/d4rksp4rt4n/nukige-site/contents/cache/patches_data.json"
            RESPONSE=$(curl -s -H "Authorization: token $TOKEN" \
              -H "Accept: application/vnd.github.v3+json" \
              "$API_URL")
            DOWNLOAD_URL=$(echo "$RESPONSE" | jq -r '.download_url')
            if [ "$DOWNLOAD_URL" = "null" ]; then
              echo "API response invalid (no download_url). Retrying..."
              sleep 5
              continue
            fi
            if curl -L -H "Authorization: token $TOKEN" \
                 "$DOWNLOAD_URL" \
                 -o database/data/patches_data.json; then
              echo "Downloaded successfully on attempt $attempt"
              break
            fi
            echo "Fetch attempt $attempt failed. Retrying in 5s..."
            sleep 5
          done
          if [ ! -f database/data/patches_data.json ] || [ ! -s database/data/patches_data.json ]; then
            echo "Fetch failed after 3 attempts or file empty."
            exit 1
          fi
          echo "Downloaded: $(du -h database/data/patches_data.json | cut -f1)"

      - name: Debug patches_data.json
        run: |
          ls -la database/data/patches_data.json
          head -n 5 database/data/patches_data.json 2>/dev/null || echo "File empty/invalid"
          jq empty database/data/patches_data.json && echo "JSON is valid" || echo "JSON invalid"

      - name: Run GDrive DB Builder
        run: python database/gdrive_db.py

      - name: Run Unify Databases
        run: python database/unify_db.py

      - name: Minify Database JSON
        run: python minify_db.py

      - name: Commit & Push if changed  # Conditional: Only commit on diffs
        id: commit
        run: |
          git config user.email "action@github.com"
          git config user.name "GitHub Action"
          git add database/data/
          if git diff --staged --quiet; then
            echo "changes=false" >> $GITHUB_OUTPUT
            echo "No changes to commit"
            exit 0
          fi
          echo "changes=true" >> $GITHUB_OUTPUT
          git commit -m "Auto-update database [$(date +'%Y-%m-%d %H:%M')]"
          echo "Git changes detected: ${{ steps.commit.outputs.changes }}"
        shell: bash

      - name: Push changes
        if: steps.commit.outputs.changes == 'true'
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          branch: main

      - name: Save cache  # UNCONDITIONAL: Always save post-scripts
        if: always()
        continue-on-error: true  # Safety: Log failure but don't break run
        uses: actions/cache/save@v4
        with:
          path: database/data/
          key: drive-state-v6-${{ hashFiles('database/gdrive_db.py', 'database/unify_db.py') }}-${{ github.run_id }}  # Unique: No more locks

      - name: Log cache save
        if: always()
        run: |
          echo "Cache save attempted (key: drive-state-v6-${{ hashFiles('database/gdrive_db.py', 'database/unify_db.py') }}-${{ github.run_id }})"
          echo "Saved files: $(ls -la database/data/ || true)"
        shell: bash

  # Optional: Weekly purge job (uncomment in 'on: schedule' above)
  purge-caches:
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 0 * * 0'  # Only on weekly cron
    steps:
      - name: Purge old caches
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Delete caches older than 7 days (adjust as needed)
          gh api -H "Accept: application/vnd.github.v3+json" \
            /repos/${{ github.repository }}/actions/caches?per_page=100 | \
            jq -r '.actions_caches[] | select(.created_at < "'$(date -d '7 days ago' -Iseconds)'") | .id' | \
            xargs -I {} gh api --method DELETE /repos/${{ github.repository }}/actions/caches/{}
          echo "Purged old caches"